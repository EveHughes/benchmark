---
title: "Accessible and Completed Datasets yet Majority are Bronze-Graded and not Updated Regularly with Missing Metadata"
subtitle: "An analysis of the data quality of datasets available on the Open Data Toronto Portal (As of May 13, 2025)"
author: 
  - Emily Su
thanks: "Code and data are available at: [https://github.com/moonsdust/data-quality](https://github.com/moonsdust/data-quality)."
date: today
date-format: long
abstract: "As one of the central hubs for Toronto-related data, we analyzed the data quality of Open Data Toronto's catalogue. Despite Open Data Toronto's extensive dataset catalogue being accessible and having minimal missing data, 56% of their datasets are graded bronze and bronze-graded datasets are less likely to be updated and have completed metadata fields. These findings can help raise awareness to Open Data Toronto whose datasets play an important role in news reporting and policymaking, and also inform anyone interested in using datasets from Open Data Toronto's catalogue about what goes behind the grade given to datasets."
format:
  pdf:
    toc: true
number-sections: true
bibliography: references.bib
execute:
  python: ".venv/bin/python"
  external: true
---
```{python}
#| include: false
#| warning: false
#| message: false

import polars as pl
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import pyarrow

# Import dataset 
cleaned_data = pl.read_csv("../data/02-analysis_data/analysis_data.csv")
cleaned_data = cleaned_data.to_pandas()
```


# Introduction

In 2024, there was a story by the Investigative Journalism Foundation (IJF) and CBC that said people in lower-income Toronto wards had more risk of dying or being injured in fires than people in higher-income wards [@citeCityFire]. This story and other stories in the media used data from Open Data Toronto to help support their stories [@citeCityFire]. Open Data Toronto is a place where lots of different kinds of data about Toronto are stored, like crime stats and shelter data [@citeODTabout]. The site is not just for news but is also used when the city makes decisions or policies [@citeODTabout]. Because of this, it’s important to think about how good the data on the site is and to ask: how good is the data that Open Data Toronto provides?

In this paper, we looked at the data quality grades on the site and some other things like “accessibility,” “completeness,” “freshness,” “metadata,” and “usability.” Farrow talked about how freshness and metadata were not very good [@citeFarrow]. But no one has really looked at what these characteristics are like for the different dataset grades in 2025. We found that all datasets are accessible, and most of them don’t have a lot of missing data and are usable. We also saw that 56% of datasets were bronze, and these datasets were less likely to be updated and didn’t have good metadata. These bronze datasets are probably the reason that freshness and metadata scores were low. These results might help Open Data Toronto know which datasets to focus on fixing and what parts of them—like metadata—they should work on. It might also help people using the portal understand how datasets are graded and what the grades actually mean.

Next in this paper, the data section (@sec-data) will show the data we used, how we got it, what it's like, the limits, and what variables we looked at. Then in the results (@sec-results) we’ll show some graphs. The discussion (@sec-discussion) will go over what we did, what it means, why it matters, and what we could have done better. At the end, the appendix (@sec-appendix) has thank-yous and extra stuff.



# Data {#sec-data}

## Overview

The dataset that we used in this paper comes from the Open Data Toronto portal and it’s called “Catalogue quality scores” [@citeOpenDataToronto]. We also looked at other datasets like “Toronto Open Data Intake” but that one didn’t really show anything about the quality of datasets people were asking for. The dataset we used is about how good or bad the datasets on Open Data Toronto are, and it helps people know which ones are more useful for stuff like news stories or other city-related things. The datasets in it are scored based on things like accessibility, completeness, freshness, metadata, and usability. These are all put together somehow to make an overall grade. You can see this grade when you look at a dataset page on the portal—it shows up as a trophy icon [@citeOpenDataToronto].

To work with the data and do everything in this paper, we used Python [@citePython] and R [@citeR] and a bunch of different libraries. Some of them were Requests [@citeRequests], datetime [@citeDatetime], Matplotlib [@citeMatplotlib], numpy [@citenumpy], pandas [@citepandas], Polars [@citepolars], Pydantic [@citePydantic], seaborn [@citeSeaborn], Pointblank [@citePointblank], and Pyarrow [@citePyarrow]. These helped with downloading, cleaning, analyzing, and testing the data and writing the paper too.

We got the dataset by using the Open Data Toronto API [@citeOpenDataToronto] and used Requests [@citeRequests] to download it as a CSV. After cleaning, the dataset had 39,580 rows, and each row is one of the datasets from the catalogue. There’s a table later on (see @tbl-dataset-preview) that shows what some of the cleaned data looks like.

```{python}
#| label: tbl-dataset-preview
#| tbl-cap: Preview of dataset on Open Data Toronto's Catalogue quality scores as of May 13, 2025
#| echo: false
#| warning: false
#| message: false

# Create table
cleaned_data[["accessibility", "completeness", "freshness", "metadata", "usability", "grade"]].head(5)
```

@tbl-summary-statistics shows the summary statistics of the cleaned dataset: 

```{python}
#| label: tbl-summary-statistics
#| tbl-cap: Summary statistics of dataset on Open Data Toronto's Catalogue quality scores as of May 13, 2025
#| echo: false
#| warning: false
#| message: false

# Create table
cleaned_data.describe()
```

## Measurement
Open Data Toronto uses something called the "Data Quality Score" to give datasets a grade like "bronze", "silver", or "gold", and you can see this grade if you go on the page for any dataset on the Open Data Toronto website. To make the "Data Quality Score", they made a group called the Data Quality Working Group. This group is made up of a bunch of different people, like people who use the data and also people who make the data [@citeCatalogue; @citeCatalogueSteps].

They first looked at a bunch of documents and papers, including academic stuff and white papers, and came up with 15 quality things to use to measure how good a dataset is [@citeCatalogue; @citeCatalogueSteps]. The things they picked to focus on were: Interpretability (which is basically how easy it is to understand the data), Usability (how easy to work with), Metadata (is it described well), Freshness (how new it is), Granularity (how detailed), Completeness (if anything is missing), and Accessibility (how easy it is to access) [@citeCatalogue; @citeCatalogueSteps].

After that, they gave a survey to the group to rank how important each of the things were, like from 1 to 7, where 1 is the most important one and 7 is the one they care about the least. These rankings were used to come up with how much each thing counts toward the score. Some of the things were taken out or mixed together. For example, granularity was taken out, and interpretability was merged into usability. Then they used some math method called Sum and Reciprocal to decide the final weights and made a table showing the weights and the questions they asked [@citeCatalogue; @citeCatalogueSteps].

If you want to know more technical stuff about how all the dimensions are actually calculated, you can go to this link: https://github.com/open-data-toronto/framework-data-quality/blob/master/data_quality_score.ipynb [@citeDQSNotebook]. But basically, the score is based on things in the metadata and the actual dataset. If the total score is below 60%, the dataset gets a Bronze grade, and if it’s between 60% and 80% it gets Silver, and above 80% it gets Gold. The Open Data Toronto team uses CKAN’s API to collect all the datasets and score them [@citeCatalogue; @citeCatalogueSteps]. These scores get updated every single week [@citeCatalogue].

## Variables of Interest

The variables we looked at in our analysis were: "accessibility", "completeness", "freshness", "metadata", "usability", and "grade". “Accessibility” is a number from 0 to 1 that tells how easy it is to get the dataset using the API, keywords, or other stuff like that. If the score is 1, then it’s good, if it’s 0, then it’s not. “Completeness” is also 0 to 1 and tells how much data is missing. A 1 means nothing is missing, and a 0 means all of it is missing. “Freshness” is how new the data is, and it checks the time between updates—shorter time = higher score. “Metadata” looks at how many of the metadata fields are filled in, like the description, limitations, contact email, and topics. The more fields filled out, the better. “Usability” is also from 0 to 1 and tells if the dataset is easy to use by checking how many column names are actual English words. One issue is that this doesn't work well if the dataset is in a different language, which kind of messes up the score.

# Results {#sec-results}

## Grade and accessibility of datasets 
```{python}
#| label: fig-grade-accessibility-bar
#| fig-cap: "Number of datasets and their accessibility on Open Data Toronto graded bronze, silver, and gold as of May 13, 2025"
#| warning: false
#| message: false
#| echo: false

# Set styling
sns.set_theme(style="whitegrid")
plt.figure(figsize=(12, 6))

# Code from: https://stackoverflow.com/questions/49044131/how-to-add-data-labels-to-seaborn-countplot-factorplot

# Create countplot
ax = sns.countplot(cleaned_data, x="grade",
                   order=cleaned_data["grade"].value_counts(ascending=False).index, palette = "crest", hue = "accessibility");
# Create label
lbls = [f'{p[0]} ({p[1]:.0f}%)' for p in zip(cleaned_data['grade'].value_counts(ascending=False), cleaned_data['grade'].value_counts(ascending=False, normalize=True).values * 100)]
# Combine countplot and labels into bar_label
ax.bar_label(container=ax.containers[0], labels=lbls)

# Add labels and title
plt.xlabel("Grade of the Dataset")
plt.ylabel("Number of Datasets")

# Adjust layout to prevent clipping of tick-labels
plt.tight_layout()

# Display the plot
plt.show()
```

As of May 13, 2025, @fig-grade-accessibility-bar shows that 56% of datasets on the Open Data Toronto portal had a grade of "bronze". Following this, 25% of datasets are graded "gold" and finally 19% of datasets are graded "silver". This means half of the datasets on the Open Data Toronto portal are ranked "bronze". However since all the datasets have an accessibility score of 1, which indicates they are accessible, and the mean accessibility score is 1 by @tbl-summary-statistics as well, it indicates all datasets on the Open Data Toronto portal can be accessed directly using methods like an API, tags or keywords, or automated data pipelines accessing Open Data Toronto's catalogue.

## The relationship between completeness and usability scores of datasets 
```{python}
#| label: fig-completeness-usability-scatterplot
#| fig-cap: "The relationship between completeness scores and usability scores of Open Data Toronto's datasets across different grades as of May 13, 2025"
#| warning: false
#| message: false
#| echo: false

# Set styling
sns.set_theme(style="whitegrid")

# Create lmplot
ax = sns.lmplot(data = cleaned_data, x = "completeness", y = "usability", col = "grade", hue="grade", palette="crest", ci=None,
    height=4, scatter_kws={"s": 50, "alpha": 0.2})
    
# Add labels and title
ax.set(xlabel="Completeness of the Dataset", ylabel="Usability of the Dataset")

# Adjust layout to prevent clipping of tick-labels
plt.tight_layout()

# Display the plot
plt.show()
```

As seen in @fig-completeness-usability-scatterplot, for all grades, there's a slight positive relationship between the completeness of a dataset on the Open Data Toronto portal and its usability. However, this relationship is more apparent with the datasets that are graded bronze. This means as the completeness score increases, the usability score of the dataset increases. We can also see most of the scores for bronze-graded datasets are more spread out along the completeness score axis compared to gold-graded and silver-graded datasets. This indicates that more bronze-grade datasets contain more missing data than the other graded datasets. Despite this, @fig-completeness-distribution shows that the completeness score of datasets on Open Data Toronto skews left with their peaks being above 0.6 (60%), this indicates that across all grades, the datasets have minimal missing data. @tbl-summary-statistics also indicates that the mean values for completeness and usability scores across all datasets are 0.87 (87%) and 0.84 (84%), respectively. 

```{python}
#| label: fig-completeness-distribution
#| fig-cap: "The distribution of completeness scores of Open Data Toronto's datasets across different grades as of May 13, 2025"
#| warning: false
#| message: false
#| echo: false

# Set styling
sns.set_theme(style="whitegrid")

# Create density plot
ax = sns.kdeplot(
   data=cleaned_data, x="completeness", hue="grade",
   fill=True, common_norm=False, palette="crest",
   alpha=.3, linewidth=1.3,
)

# Add labels and title
plt.xlabel("Completeness Score of the Dataset")
plt.ylabel("Number of Datasets")

# Adjust layout to prevent clipping of tick-labels
plt.tight_layout()

# Display the plot
plt.show()
```


## Metadata completeness scores of datasets
```{python}
#| label: fig-metadata-distribution
#| fig-cap: "The distribution of metadata completeness scores of Open Data Toronto's datasets across different grades as of May 13, 2025"
#| warning: false
#| message: false
#| echo: false

# Set styling
sns.set_theme(style="whitegrid")

# Create density plot
ax = sns.kdeplot(
   data=cleaned_data, x="metadata", hue="grade",
   fill=True, common_norm=False, palette="crest",
   alpha=.3, linewidth=1.3,
)

# Add labels and title
plt.xlabel("Metadata Completeness Score of the Dataset")
plt.ylabel("Number of Datasets")

# Adjust layout to prevent clipping of tick-labels
plt.tight_layout()

# Display the plot
plt.show()
```

@fig-metadata-distribution shows that the metadata score for all datasets of Open Data Toronto's datasets has a multimodal distribution. However, the distribution of gold-graded datasets skew left overall. This means that most of the gold-graded datasets have metadata that is almost or is completed filled on the Open Data Toronto portal. On the other hand, the distribution of bronze-graded datasets overall skew right with its largest peak being below a metadata score of 0.5 or 50%. This indicates that the metadata fields for bronze-graded datasets are not sufficiently field or yet not filled out on the Open Data Toronto portal. @tbl-summary-statistics indicates that the mean metadata completeness score is 0.47 (47%) for all datasets on the portal. This means that the average metadata completeness score is below 50% for all datasets on the portal. 

## Freshness scores of datasets 
```{python}
#| label: fig-freshness-distribution
#| fig-cap: "The distribution of freshness scores of Open Data Toronto's datasets across different grades as of May 13, 2025"
#| warning: false
#| message: false
#| echo: false

# Set styling
sns.set_theme(style="whitegrid")

# Create density plot
ax = sns.kdeplot(
   data=cleaned_data, x="freshness", hue="grade",
   fill=True, common_norm=False, palette="crest",
   alpha=.3, linewidth=1.3,
)

# Add labels and title
plt.xlabel("Freshness Score of the Dataset")
plt.ylabel("Number of Datasets")

# Adjust layout to prevent clipping of tick-labels
plt.tight_layout()

# Display the plot
plt.show()
```

As of May 13, 2025, @fig-freshness-distribution indicates that for gold-graded and silver-graded datasets, their distributions skews left and that the highest peaks of their distributions are around a freshness score of 1.0 or 100%. This indicates that the datasets that are gold-graded and silver-graded are frequently updated. However with bronze-graded datasets, its distribution skews right with its highest peak being around a freshness score 0.0 or 0%. This indicates that the datasets are not updated frequently or at all. @tbl-summary-statistics also shows that the mean freshness score is 0.56 (56%) across all datasets.  

\newpage

# Discussion {#sec-discussion}
In @sec-results, we looked at the data quality of 39,580 datasets on the Open Data Toronto as of May 13, 2025 and the different characteristics of the datasets. We found that our analysis was consistent with what @citeFarrow found where the metadata and freshness scores of datasets overall was poor but also we found that the low scores were contributed from the bronze-graded datasets. 

## Majority of Datasets are graded "Bronze" 
We saw with that with @fig-grade-accessibility-bar that 56% of datasets in the Open Data Toronto portal are graded "Bronze". This indicates that 56% of datasets had a data quality score of less than 60%. This also raises concerns regarding the quality of the datasets used in news report for example as well as bring awareness of the quality of datasets currently on the portal. Fortunately based on what @citeFarrow found in comparison to our results in 2025, there has a decrease in bronze-graded datasets since 2021 from 78% to 56%. 

## Bronze-graded datasets are less likely to update and have missing metadata 
Our results from @fig-metadata-distribution and @fig-freshness-distribution shows that bronze-graded datasets have low metadata and freshness scores close to 0 or 0%. This indicates that the bronze-graded datasets contributes to the low metadata and freshness score seen of Open Data Toronto's entire data catalogue. As noted by IBM, Metadata plays an important role in "data governance and data management" [@citeMetadataImportance]. This means that the lack of metadata for a dataset decreases the experience for users and organization of using the datasets leading to potentially consequences due to issues such as the lack of information of the dataset's limitations and the lack of information about the dataset author's and their contact information. 

## Areas of improvement 
As mentioned in @sec-data, datasets have a higher usability score if their column names contains more English words. However, this criteria does not consider datasets that could be still useful but are not in English. Another limitation of our analysis is that the weight of the different dimensions in the data that goes towards the grade of a dataset is subjective in nature since the weighing is based on survey data that had people rank the perceived importance of each dimension. 

## Next steps
Results from our analysis can be used help the Open Data Toronto team figure out the qualities of bronze-graded datasets that leads to their low scores and also be insight for users of the datasets from Open Data Toronto about what goes behind the grade of the datasets. Future works regarding Open Data Toronto's catalogue can look into the data quality score of the datasets from different divisions.

\newpage

\appendix

# Appendix {#sec-appendix}

## Acknowledgments

We would like to thank @tellingstories for providing assistance with the code used to produce the graphs in this paper. We would also like to thank the team at the IJF for their feedback. 

# References


